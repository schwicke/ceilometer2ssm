#!/usr/bin/env python
import httplib
import urllib
import json
import cStringIO
import re
import os
import sys
import urlparse
import argparse
import subprocess
import shlex
import logging

from time import localtime, strftime
from pwd import getpwnam  
from dateutil import parser
from dirq.QueueSimple import QueueSimple

state_map = {
    # from https://wiki.egi.eu/wiki/Fedcloud-tf:WorkGroups:Scenario4
    'active': 'started',
    'build': 'started',
    'deleted': 'completed',
    'error': 'error',
    'hard_reboot': 'started',
    'migrating': 'started',
    'paused': 'paused',
    'reboot': 'started',
    'rebuild': 'started',
    'confirming_resize': 'started',
    'rescue': 'started',
    'resize': 'started',
    'revert_resize': 'started',
    'password': 'started',
    'verify_resize': 'started',
    'shutoff': 'completed',
    'suspended': 'suspended',
    'terminated': 'completed',
    'stopped': 'completed',
    'saving': 'started'
    }


def read_config(filename):
   # read the mapping of project-id to accounting group from a file
   try:
      f = open(filename,"r")
      try:
         result=json.loads(f.read())
         f.close
         return result
      except:
         logging.error("Cannot parse configuration file %s" % filename)
   except IOError:
      logging.error('Cannot open configuration file %s' % filename)
   sys.exit(1)
        

def auth_keystone(keystoneserver, username, password, tenant, cacert):
   auth = '{"auth":{"passwordCredentials":{"username":"%s", "password":"%s"}, "tenantName":"%s"}}' % (username, password, tenant)
   auth_server = urlparse.urlparse(keystoneserver)[1]
   auth_protocol = urlparse.urlparse(keystoneserver)[0]
   auth_path = urlparse.urlparse(keystoneserver)[2]
   if auth_protocol == "https":
      Newconn = httplib.HTTPSConnection
   else:
      Newconn = httplib.HTTPConnection

   header = {'Content-type': 'application/json'}
   req = 'POST'
   get_auth_conn = Newconn(auth_server)
   try:
      get_auth_conn.request(req, auth_path, auth, header)
      answer = get_auth_conn.getresponse()
      if answer.status == 200:
         logging.debug('Keystone authentication succeeded')
         return answer.read()
      else:
         logging.error("Failed to get keystone token from %s" % keystone_server)
         logging.error(answer.reason)
   except:
      logging.error('An error occurred while connecting to keystone server')
      sys.exit(1)

def receive_data(keystone_response, start, end, meter):
   decoded = json.loads(keystone_response)
   tokenid = decoded['access']['token']['id']
   uri = '/v2/meters/%s' % meter
   for endpoint in decoded['access']['serviceCatalog']:
      if endpoint['name'] == 'ceilometer':
         for ceilometers in endpoint['endpoints']:
            ceilometer_server = urlparse.urlparse(ceilometers['publicURL'])[1]
            ceilometer_protocol = urlparse.urlparse(ceilometers['publicURL'])[0]
            if ceilometer_protocol == 'https':
               NEWconn = httplib.HTTPSConnection
            else:
               NEWconn = httplib.HTTPConnection
               query = {'q':[{'field':'timestamp',
                              'op':'ge',
                              'value':start},
                             {'field': 'timestamp',
                              'op':'le',
                              'value':end}
                            ]
                       }
               data = json.dumps(query)
               header = {'Content-type': 'application/json',
                         'X-Auth-Token': tokenid.encode('ascii', 'ignore')
                        }
               req = 'GET'
                
               logging.debug("Query ceilometer server at: %s" % ceilometer_server)
               logging.debug("Query it with: :%s" % json.dumps(query, indent=2))
                
               try:
                  get_cm_conn = NEWconn(ceilometer_server)
                  try:
                     get_cm_conn.request(req, uri, body=data, headers=header)
                     res = get_cm_conn.getresponse()
                     if res.status == 200:
                        return json.loads(res.read())
                  except:
                     logging.error("Failed to query ceilometer server at %s" % ceilometers)
                     sys.exit(1)
               except:
                  logging.error('failed to create the connection object')


def get_uid(username):
   uid = 0
   try:
      uid = getpwnam(username).pw_uid
   except:
      logging.error("Cannot find uid for %s" % username)
      uid = getpwnam('nobody').pw_uid
   return uid 


def get_accgroup(filter, mapping, report_groups, project):
   # return the accounting group
   try:
      gid = mapping[project]['group']
      if filter:
         try: 
            report_groups.index(gid)
         except:
            gid = 'unset'
   except:
      gid = 'unknown'
   return gid 


def get_tenant(mapping, project):
   # return the accounting group
   try:
      tenant = mapping[project]['name']
   except:
      tenant = 'NULL'
   return tenant 


def ana_received_cpu_data(filter, mapping, report_groups, cpu_data, hide_names):
   #
   # filter for start and end records
   #
   try:
      for record in cpu_data:
         gid = get_accgroup(filter, mapping, report_groups, record['project_id'])
         logging.debug("Processing resource_id %s in project %s accounting group: %s" % (record['resource_id'], record['project_id'], gid) )
            
         if gid == 'unset' or (filter and (gid == 'NULL')):
            logging.debug("Skipped input record = %s" % json.dumps(record , indent=2))   
         else:
            ssm_record = generate_ssm_record(record, mapping, gid, hide_names)
   except TypeError:
      logging.error('No CPU usage data information has been received')
      sys.exit(1)
   return ssm_record


def generate_ssm_record(record, mapping, gid, hide_names):
   ssm_record = {}
   
   resource_id = record['resource_id']
   timestamp = parser.parse(record['timestamp']).strftime("%s")
   # memorize relevant data and find start and end record
   if resource_id in ssm_record:
      ssm_record[resource_id]
   else:
      logging.debug("New record %s" % resource_id)
      ssm_record[resource_id] = {'periodstart' : timestamp,
                                 'periodend': timestamp,
                                 'machinename': 'NULL',
                                 'state' : 'NULL',
                                 'imageid' : 'NULL',
                                 'cpucount': None
                                }
                
   if timestamp < ssm_record[resource_id]['periodstart']:
      ssm_record[resource_id]['periodstart'] = timestamp
   elif timestamp >= ssm_record[resource_id]['periodend']:
      logging.debug("Updating record %s start: %s end : %s" % (resource_id, ssm_record[resource_id]['periodstart'], timestamp))
                     
      ssm_record[resource_id]['periodend'] = timestamp
      ssm_record[resource_id]['tenant'] = get_tenant(mapping, record['project_id'])
      ssm_record[resource_id]['vmuuid'] = resource_id
      ssm_record[resource_id]['uid'] = record['user_id']
      ssm_record[resource_id]['gid'] = gid 
      ssm_record[resource_id]['vcpu'] = record['resource_metadata']['vcpus']
      ssm_record[resource_id]['memory'] = record['resource_metadata']['memory_mb']
      ssm_record[resource_id]['disk'] = record['resource_metadata']['disk_gb']
               
      generate_optional_fields_ssm_record(ssm_record[resource_id],record)               
      hide_names_from_ssm_record(ssm_record[resource_id], record, hide_names)
   return ssm_record


def generate_optional_fields_ssm_record(ssm_record , record):
   if 'image_ref_url' in record['resource_metadata']:
      ssm_record['imageid'] = record['resource_metadata']['image_ref_url']
                        
   if 'state' in record['resource_metadata']:
      ssm_record['state'] = state_map[record['resource_metadata']['state']]
                        
   if 'created_at' in record['resource_metadata']:
      ssm_record['starttime'] = parser.parse(record['resource_metadata']['created_at']).strftime("%s")                          
                        
   if 'deleted_at' in record['resource_metadata']:
      # ssm_record[resource_id]['endtime'] = parser.parse(record['resource_metadata']['deleted_at']).strftime("%s")
      endtime = parser.parse(record['resource_metadata']['deleted_at']).strftime("%s")
      # machine terminated 
      ssm_record['state'] = 'completed'
                          
      if 'endtime' in ssm_record and ssm_record['endtime'] > endtime:  
         ssm_record['endtime'] = endtime
      else:
         ssm_record['endtime'] = endtime
                        
   if record['counter_unit'] == 'ns':
      ssm_record['cpucount'] = record['counter_volume']
   else:
      logging.error("Unknown counter unit type %s" % record['counter_unit'])   


def hide_names_from_ssm_record(ssm_record, record , hide_names):
   if hide_names:
      if 'instance_id' in record['resource_metadata']:
         ssm_record['machinename'] = record['resource_metadata']['instance_id']
         try:
            ssm_record['uid'] = str(get_uid(uid))
         except:
            ssm_record['uid'] = "nobody"
   else:
      if 'display_name' in record['resource_metadata']:
         ssm_record['machinename'] = record['resource_metadata']['display_name']
         
         
def ana_received_net_data(ssm_record, filter, mapping, report_groups, net_data):
   #
   # filter for start and end records
   #
   try:
      for record in net_data:
         gid = get_accgroup(filter, mapping, report_groups, record['project_id'])
         if not (gid == 'unset' or (filter and (gid == 'NULL'))):
            # memorize relevant data and find start and end record
            resource_id = record['resource_metadata']['instance_id']
            timestamp = record['timestamp']
            counter_name = record['counter_name']
            if record['counter_unit'] == 'B':
               netcount = record['counter_volume']
            else:
               logging.error("Unknown counter unit type %s" % record['counter_unit'])
                    
            if counter_name in ssm_record[resource_id]:
               if (timestamp < ssm_record[resource_id][counter_name]['periodstart']):
                  ssm_record[resource_id][counter_name]['periodstart'] = timestamp
               if (timestamp > ssm_record[resource_id][counter_name]['periodend']):
                  ssm_record[resource_id][counter_name]['periodend'] = timestamp
                  ssm_record[resource_id][counter_name]['counter_value'] = netcount                            
            else:
               ssm_record[resource_id][counter_name] = {}
               ssm_record[resource_id][counter_name]['periodstart'] = timestamp                    
               ssm_record[resource_id][counter_name]['periodend'] = timestamp
               ssm_record[resource_id][counter_name]['counter_value'] = netcount
   except TypeError:
      logging.error('No network usage data information has been received')
      sys.exit(1)
   return ssm_record


def print_ssm_records(ssm, sitename):
   line = "APEL-cloud-message: %s\n" % "v0.2"
   for resource_id in ssm.keys():
      if ssm[resource_id]:
         if 'periodstart' in ssm[resource_id] and 'periodend' in ssm[resource_id]:
            logging.info("reported period: from %s to %s" % (ssm[resource_id]['periodstart'], ssm[resource_id]['periodend']))
         else:
            logging.error('--------------------')
            logging.error("cannot find periodstart for resource %s" % resource_id)
            logging.error(json.dumps(ssm[resource_id], indent=2))
            logging.error('--------------------')
               
         line += "VMUUID: %s\n" % ssm[resource_id]['vmuuid']
         line += "SiteName: %s\n" % sitename
         line += "MachineName: %s\n" % ssm[resource_id]['machinename']
         line += "LocalUserId: %s\n" % str(ssm[resource_id]['uid'])
         line += "LocalGroupId: %s\n" % str(ssm[resource_id]['gid'])
         line += "GlobalUserName: NULL\n"
         line += "FQAN: NULL\n"
            
         if ssm[resource_id]['state'] == "running":
            line += "Status: %s\n" % "NULL"
         else:
            line += "Status: %s\n" % ssm[resource_id]['state']
            
         if 'starttime' in ssm[resource_id]:
            starttime = int(ssm[resource_id]['starttime'])
         else:
            starttime = 0
            logging.debug('Starttime is not set. Skipping')
         line += "StartTime: %d\n" % starttime    
            
         if 'endtime' in ssm[resource_id]:
            endtime = int(ssm[resource_id]['endtime'])
         else:
            endtime = 0
         line += "EndTime: %d\n" % int(endtime)

         if endtime > 0 and starttime > 0:
            walltime = endtime - starttime
         else:
            walltime = int(ssm[resource_id]['periodend']) - starttime

         line += "WallDuration: %d\n" % int(walltime)
         line += "CpuDuration: %d\n" % int(0.5 + float(ssm[resource_id]['cpucount']) / 1000000000.0)
            
         if 'vcpu' in ssm[resource_id]:
            line += "CpuCount: %d\n" % int(ssm[resource_id]['vcpu'])
         else:
            line += "CpuCount: %d\n" % 0
            logging.debug('Cannot get cpu count. Skipping')
            logging.debug(ssm[resource_id]['vcpu'])
            
         line += "%s\n" % 'NetworkType: NULL'
            
         if 'network.incoming.bytes' in ssm[resource_id]:
            line += "NetworkInbound: %d\n" % int(0.5 + float(ssm[resource_id]['network.incoming.bytes']['counter_value']) / 1073741824.0)
         else:
            logging.debug('Inbound traffic is not set. Skipping')
            
         if 'network.outgoing.bytes' in ssm[resource_id]:
            line += "NetworkOutbound: %s\n" % int(0.5 + float(ssm[resource_id]['network.outgoing.bytes']['counter_value']) / 1073741824.0)
         else:
            logging.debug('Outbound traffic is not set. Skipping')
            
         if 'memory' in ssm[resource_id]:
            line += "Memory: %d\n" % int(ssm[resource_id]['memory'])
         else:
            logging.debug('Memory is not set. Skipping')
            logging.debug(ssm[resource_id]['memory'])
            
         if 'disk' in ssm[resource_id]:
            line += "Disk: %d\n" % int(ssm[resource_id]['disk'])
         else:
            logging.debug('Disk is not set. Skipping')
         # to be added later on
         # line += "GlobalUserName: %s\n" % "NULL"
         # line += "%s\n" % "FQAN: NULL"
         # line += "SuspendDuration: %s\n" % "NULL"
         line += "StorageRecordId: %s\n" % "NULL"
         line += "ImageId: %s\n" % str(ssm[resource_id]['imageid'])
         line += "CloudType: %s\n" % "OpenStack"
         line += "%s\n" % "%%"
   return line

def create_report(ssm):
   report = "Last Update: %s\n" % strftime("%a, %d %b %Y %H:%M:%S", localtime())
   report += '| *Accounting group* | *VMs* | *CPUs* | *disk* | *cpu time* | *memory* | *net in* | *net out* | \n'
   by_accgroup = {}
   for resource_id in ssm.keys():
      if ssm[resource_id]['gid'] not in by_accgroup:
         by_accgroup[ssm[resource_id]['gid']] = {'nvms': 0,
                                                 'ncores': 0,
                                                 'disk': 0,
                                                 'cpucount': 0,
                                                 'memory': 0,
                                                 'net_in': 0,
                                                 'net_out':0
                                                }
        
      by_accgroup[ssm[resource_id]['gid']]['nvms'] += 1
      by_accgroup[ssm[resource_id]['gid']]['ncores'] += int(ssm[resource_id]['vcpu'])  
      by_accgroup[ssm[resource_id]['gid']]['disk'] += int(ssm[resource_id]['disk'])
      by_accgroup[ssm[resource_id]['gid']]['cpucount'] += int(ssm[resource_id]['cpucount'] / 1000000000.0)  
      by_accgroup[ssm[resource_id]['gid']]['memory'] += int(ssm[resource_id]['memory'])
      by_accgroup[ssm[resource_id]['gid']]['net_in'] += float(ssm[resource_id]['network.incoming.bytes']['counter_value'] / 1073741824.0)  
      by_accgroup[ssm[resource_id]['gid']]['net_out'] += float(ssm[resource_id]['network.outgoing.bytes']['counter_value'] / 1073741824.0) 
        
   for accgroup in by_accgroup.keys():
      report += "| %s | %s | %s | %s | %s | %s | %s | %s |\n" % (accgroup, 
                                                                 str(by_accgroup[accgroup]['nvms']),
                                                                 str(by_accgroup[accgroup]['ncores']),
                                                                 str(by_accgroup[accgroup]['disk']),
                                                                 str(by_accgroup[accgroup]['cpucount']),
                                                                 str(by_accgroup[accgroup]['memory']),
                                                                 str(by_accgroup[accgroup]['net_in']),
                                                                 str(by_accgroup[accgroup]['net_out']))
   return report


def parse_and_return_arguments():
   aparser = argparse.ArgumentParser(description='Publish ceilometer records to APEL using SSM2')
   aparser.add_argument('-p',
                        '--publish',
                        dest='publish',
                        action='store_true',
                        help='directly publish the data',
                        default=False)
   aparser.add_argument('-v',
                        '--verbose',
                        dest='verbose',
                        action='store_true',
                        help='be verbose',
                        default=False)
   aparser.add_argument('-d',
                        '--debug',
                        dest='debug',
                        action='store_true',
                        help='produce debugging output',
                        default=False)
   aparser.add_argument('-s',
                        '--start',
                        dest='start',
                        action='store',
                        help='start time for the publication',
                        default="2013-09-20T00:00:00")
   aparser.add_argument('-e',
                        '--end',
                        dest='end',
                        action='store',
                        help='end time for the publicatin',
                        default="2013-09-20T23:59:59")
   aparser.add_argument('-c',
                        '--config',
                        dest='configfile',
                        action='store',
                        help='ceilometer2ssm configuration file location',
                        default="/etc/ceilometer2ssm.conf")
   aparser.add_argument('-a',
                        '--apelssmconfig',
                        dest='apelssmconf',
                        action='store',
                        help='location of the apel-ssm configuration file',
                        default='/etc/apel/sender.cfg')
   aparser.add_argument('-l',
                        '--localreport',
                        dest='localreport',
                        action='store_true',
                        help='Create also a local report in Twiki format. Implies --nofilter',
                        default=False)
   aparser.add_argument('-n',
                        '--nofilter',
                        dest='nofilter',
                        action='store_true',
                        help='Do not filter the output for groups',
                        default=False)
   
   args = aparser.parse_args()
   
   return (args.start,
           args.end,
           args.publish,
           args.verbose,
           args.debug,
           args.configfile,
           args.apelssmconf,
           args.localreport,
           args.nofilter
                     )
   
def read_data_from_secrets(config):
   secrets = {}
   os_auth_url = None
   os_username = None
   os_password = None
   os_tenant_name = None
   os_cacert = None
   
   if 'secrets' in config:
      secrets = config['secrets']
   else:
      logging.info('No secrets defined in the configuration file')
    
   if 'os_auth_url' in secrets:
      os_auth_url = secrets['os_auth_url']
   elif 'OS_AUTH_URL' in os.environ:
      os_auth_url = os.environ['OS_AUTH_URL']
   else:
      raise Exception('OS_AUTH_URL is not set')
       
   if 'os_username' in secrets:
      os_username = secrets['os_username']
   elif 'OS_USERNAME' in os.environ:
      os_username = os.environ['OS_USERNAME']
   else:
      raise Exception('OS_USERNAME is not set')
   
   if 'os_password' in secrets:
      os_password = secrets['os_password']
   elif 'OS_PASSWORD' in os.environ:
      os_password = os.environ['OS_PASSWORD']
   else:
      raise Exception('OS_PASSWORD is not set')
   
   if 'os_tenant_name' in secrets:
      os_tenant_name = secrets['os_tenant_name']
   elif 'OS_TENANT_NAME' in os.environ:
      os_tenant_name = os.environ['OS_TENANT_NAME']
   else:
      raise Exception('OS_TENANT_NAME is not set')
   
   if 'os_cacert' in secrets:
      os_cacert = secrets['os_cacert']
   elif 'OS_CACERT' in os.environ:
      os_cacert = os.environ['OS_CACERT']
   else:
      raise Exception('OS_CACERT is not set')
         
   return (os_auth_url,
           os_username,
           os_password,
           os_tenant_name,
           os_cacert)
         

def retrieve_data(keystone_response, start , end ):
   logging.info('reading data from ceilometer')
   cpu_used = receive_data(keystone_response, start, end, 'cpu')
   net_in = receive_data(keystone_response, start, end, 'network.incoming.bytes')
   net_out = receive_data(keystone_response, start, end, 'network.outgoing.bytes')
   
   return (cpu_used,
           net_in,
           net_out)

def analyse_data(filtered, mapping, report_groups, hide_names, cpu_used, net_in, net_out):
   logging.info('analysing data')
   ssm_filtered = ana_received_cpu_data(filtered, mapping, report_groups, cpu_used, hide_names)
   ssm_filtered = ana_received_net_data(ssm_filtered, filtered, mapping, report_groups, net_in)
   ssm_filtered = ana_received_net_data(ssm_filtered, filtered, mapping, report_groups, net_out)  
   return ssm_filtered
   
def print_local_report(mapping, report_groups, hide_names, cpu_used, net_in, net_out):
   logging.info('analysing all data')
   ssm_full = analyse_data(True, mapping, report_groups, hide_names, cpu_used, net_in, net_out)
   twiki_report = CreateReport(ssm_full)
   print twiki_report
   
def publish_records(debug, apelssmconf):
   dirq = QueueSimple('/var/spool/apel/outgoing/')
   dirq.add(records)
   command_line = "/usr/bin/ssmsend --config %s" % apelssmconf
   if (debug):
      logging.debug("Would now run \"%s\"" % command_line)
   else:
      args = shlex.split(command_line)
      try:
         p = subprocess.Popen(args)
         if (p.wait() != 0):
            logging.error(p)
            sys.exit(1)
      except:
         logging.error('Failed to send the message')
         sys.exit(1)

                          
if __name__ == '__main__':
    
    (start, end, publish, verbose, debug, configfile, apelssmconf, localreport, nofilter) = parse_and_return_arguments()
     
    if verbose:
       logging.basicConfig(level=logging.INFO)
    
    if debug:
       logging.basicConfig(level=logging.DEBUG)

    logging.debug('Debug mode is enabled: will not actually publish but just retrieve the data and report!')

    logging.info('Verbose output will be created')
    logging.info("Reading configuration from %s" % configfile)
    logging.info("Records are processed between %s and %s" % (start, end))
    if (publish):
       logging.info('Resulting records will be published to APEL')
    else:
       logging.info('Will not try to publish the result')

    # read mapping from file
    config = read_config(configfile)
    mapping = config['mapping']
    sitename = config['sitename']
    report_groups = config['report_groups']
    hide_names = config['hide_names']
    
    try:
       (os_auth_url, os_username, os_password, os_tenant_name, os_cacert) = read_data_from_secrets(config)
    except Error,e:
       logging.error(e)
       sys.exit(1)
       
    logging.debug('Getting authentication token from keystone')

    auth_keytone_first_parameter = "%s/tokens" % os_auth_url
    keystone_response = auth_keystone(auth_keytone_first_parameter, os_username, os_password, os_tenant_name, os_cacert)

    logging.debug('Keystone response:')
    logging.debug("%s" % json.dumps(keystone_response, indent=2))
    logging.debug('reading cpu and net information from ceilometer')

    #
    # retrieve data and analyse
    #

    # cpu
    (cpu_used, net_in, net_out) = retrieve_data(keystone_response, start, end)
    ssm_filtered = analyse_data(False, mapping, report_groups, hide_names, cpu_used, net_in, net_out)
    

    #
    # print the result
    #
    records = print_ssm_records(ssm_filtered, sitename)
    logging.info(records)

    if (localreport):
       print_local_report(mapping, report_groups, hide_names, cpu_used, net_in, net_out)    

    if (publish):
       publish_records(debug, apelssmconf)
            
